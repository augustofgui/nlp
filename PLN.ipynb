{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cpiqAw49kBbL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O96EbD2V4g92",
        "outputId": "dbe0ebd3-55a2-4ccd-f1db-8230f87ec943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting pyalex\n",
            "  Downloading pyalex-0.14-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from pyalex) (1.26.5)\n",
            "Requirement already satisfied: requests in /home/augustofgui/.local/lib/python3.10/site-packages (from pyalex) (2.31.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pyalex) (2020.6.20)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pyalex) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/augustofgui/.local/lib/python3.10/site-packages (from requests->pyalex) (3.3.2)\n",
            "Installing collected packages: pyalex\n",
            "Successfully installed pyalex-0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install pyalex\n",
        "import pyalex\n",
        "pyalex.config.email = \"augusto.guilarducci@aluno.ufop.edu.br\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tpz8X_4Gn7NS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('input/authors.csv')\n",
        "df_cleaned = df.dropna(subset=['institution_id', 'phd_institution_id', 'phd_institution_name'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "agArnyrR3ZEW"
      },
      "outputs": [],
      "source": [
        "authors = {}\n",
        "institutions = {}\n",
        "\n",
        "inst_arr = list(np.unique(df_cleaned[['institution_id', 'phd_institution_id']].values))\n",
        "for id in inst_arr:\n",
        "  if id == 'I1118542' or id == 'I122140585' or id == 'I122140586':\n",
        "    continue\n",
        "  json = pyalex.Institutions()[id]\n",
        "\n",
        "  institution = {\"name\": json[\"display_name\"], \"country\": json[\"geo\"][\"country\"], \"city\": json[\"geo\"][\"city\"]}\n",
        "  institution = institution | json[\"summary_stats\"]\n",
        "  institutions[id] = institution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "I4iNPFrj9G8p"
      },
      "outputs": [],
      "source": [
        "with open('./input/institutions.txt', 'w') as file:\n",
        "  initial = \"The file contains a list of institutions, detailing their name, city and country and current academic prestige metrics of h-index and i10-index. Each entry specifies the institutions informations. The h-index measures both the productivity and citation impact of a researcher’s published work. An individual has an h-index of h if they have published h papers that have each been cited at least h times. For example, an h-index of 10 means the researcher has 10 papers with at least 10 citations each. It helps to balance the number of publications with the number of citations, providing a more nuanced view of a researcher’s impact than simply counting citations or publications alone. The i10-index is simpler. It counts the number of papers with at least 10 citations. For example, if a researcher has 15 papers with at least 10 citations each, their i10-index is 15. It’s a straightforward measure of the number of influential papers.\\n\\n\"\n",
        "  file.write(initial + '\\n')\n",
        "\n",
        "\n",
        "  for id, data in institutions.items():\n",
        "    line = f\"{data['name']} is a institution in {data['city']}, {data['country']}, with an h-index score of {data['h_index']} and i10-index of {data['i10_index']}.\"\n",
        "    # print(line)\n",
        "    # break\n",
        "    file.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z20USZBGqzh_"
      },
      "outputs": [],
      "source": [
        "with open('./input/author.txt', 'w') as file:\n",
        "  initial = \"The file contains a list of professors, detailing their educational background and current academic positions. Each entry specifies the institution where the professor earned their doctorate and their current affiliation, with CAPES score. The CAPES score, issued by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES), is a quality assessment rating used in Brazil to evaluate graduate programs at universities. CAPES is a government agency under the Ministry of Education in Brazil, responsible for promoting high standards in postgraduate education. The CAPES score ranges from 1 to 7:\\n1-2: Programs with these scores are considered unsatisfactory.\\n3: The minimum acceptable level for a program to be recognized and allowed to continue operating.\\n4-5: Indicates a good to very good program, with national relevance.\\n6-7: Represents programs of excellence with international recognition and strong research output.\\nA score of 7 is the highest possible, denoting top-tier programs with outstanding academic and research quality.\"\n",
        "  file.write(initial + '\\n\\n\\n')\n",
        "  for index, row in df_cleaned.iterrows():\n",
        "    line = f\"Author {index} got their doctorate at {institutions[row['phd_institution_id']]['name']} and is a professor at {institutions[row['institution_id']]['name']}, in a Graduate Program with CAPES Score of {row['gp_score']}.\"\n",
        "\n",
        "    file.write(line + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdUUHs0ykJ9M",
        "outputId": "f34acb69-8a84-4929-a154-5d97df619349"
      },
      "outputs": [],
      "source": [
        "!pip install -q graphrag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIfN0tl0krWr",
        "outputId": "2aacf98a-37f3-455a-913f-e4117b5749c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-08-02 16:00:05.137489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-02 16:00:05.193377: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-02 16:00:05.210329: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-08-02 16:00:07.722426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2KInitializing project at .\u001b[35m/\u001b[0m\u001b[95mragtest\u001b[0m\n",
            "⠋ GraphRAG Indexer "
          ]
        }
      ],
      "source": [
        "!python -m graphrag.index --init --root ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2oVhoXUktvD",
        "outputId": "09d92bd7-b641-4999-e806-273c76421c18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./settings.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./settings.yaml\n",
        "\n",
        "encoding_model: cl100k_base\n",
        "skip_workflows: []\n",
        "llm:\n",
        "  api_key: ${GRAPHRAG_API_KEY}\n",
        "  type: openai_chat # or azure_openai_chat\n",
        "  model: gpt-4o-mini\n",
        "  model_supports_json: true # recommended if this is available for your model.\n",
        "  # max_tokens: 4000\n",
        "  # request_timeout: 180.0\n",
        "  # api_base: https://<instance>.openai.azure.com\n",
        "  # api_version: 2024-02-15-preview\n",
        "  # organization: <organization_id>\n",
        "  # deployment_name: <azure_model_deployment_name>\n",
        "  # tokens_per_minute: 150_000 # set a leaky bucket throttle\n",
        "  # requests_per_minute: 10_000 # set a leaky bucket throttle\n",
        "  # max_retries: 10\n",
        "  # max_retry_wait: 10.0\n",
        "  # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times\n",
        "  # concurrent_requests: 25 # the number of parallel inflight requests that may be made\n",
        "  # temperature: 0 # temperature for sampling\n",
        "  # top_p: 1 # top-p sampling\n",
        "  # n: 1 # Number of completions to generate\n",
        "\n",
        "parallelization:\n",
        "  stagger: 0.3\n",
        "  # num_threads: 50 # the number of threads to use for parallel processing\n",
        "\n",
        "async_mode: threaded # or asyncio\n",
        "\n",
        "embeddings:\n",
        "  ## parallelization: override the global parallelization settings for embeddings\n",
        "  async_mode: threaded # or asyncio\n",
        "  llm:\n",
        "    api_key: ${GRAPHRAG_API_KEY}\n",
        "    type: openai_embedding # or azure_openai_embedding\n",
        "    model: text-embedding-3-small\n",
        "    # api_base: https://<instance>.openai.azure.com\n",
        "    # api_version: 2024-02-15-preview\n",
        "    # organization: <organization_id>\n",
        "    # deployment_name: <azure_model_deployment_name>\n",
        "    # tokens_per_minute: 150_000 # set a leaky bucket throttle\n",
        "    # requests_per_minute: 10_000 # set a leaky bucket throttle\n",
        "    # max_retries: 10\n",
        "    # max_retry_wait: 10.0\n",
        "    # sleep_on_rate_limit_recommendation: true # whether to sleep when azure suggests wait-times\n",
        "    # concurrent_requests: 25 # the number of parallel inflight requests that may be made\n",
        "    # batch_size: 16 # the number of documents to send in a single request\n",
        "    # batch_max_tokens: 8191 # the maximum number of tokens to send in a single request\n",
        "    # target: required # or optional\n",
        "\n",
        "chunks:\n",
        "  size: 1200\n",
        "  overlap: 100\n",
        "  group_by_columns: [id] # by default, we don't allow chunks to cross documents\n",
        "\n",
        "input:\n",
        "  type: file # or blob\n",
        "  file_type: text # or csv\n",
        "  base_dir: \"input\"\n",
        "  file_encoding: utf-8\n",
        "  file_pattern: \".*\\\\.txt$\"\n",
        "\n",
        "cache:\n",
        "  type: file # or blob\n",
        "  base_dir: \"cache\"\n",
        "  # connection_string: <azure_blob_storage_connection_string>\n",
        "  # container_name: <azure_blob_storage_container_name>\n",
        "\n",
        "storage:\n",
        "  type: file # or blob\n",
        "  base_dir: \"output/${timestamp}/artifacts\"\n",
        "  # connection_string: <azure_blob_storage_connection_string>\n",
        "  # container_name: <azure_blob_storage_container_name>\n",
        "\n",
        "reporting:\n",
        "  type: file # or console, blob\n",
        "  base_dir: \"output/${timestamp}/reports\"\n",
        "  # connection_string: <azure_blob_storage_connection_string>\n",
        "  # container_name: <azure_blob_storage_container_name>\n",
        "\n",
        "entity_extraction:\n",
        "  ## llm: override the global llm settings for this task\n",
        "  ## parallelization: override the global parallelization settings for this task\n",
        "  ## async_mode: override the global async_mode settings for this task\n",
        "  prompt: \"prompts/entity_extraction.txt\"\n",
        "  entity_types: [organization, person, geo, event]\n",
        "  max_gleanings: 1\n",
        "\n",
        "summarize_descriptions:\n",
        "  ## llm: override the global llm settings for this task\n",
        "  ## parallelization: override the global parallelization settings for this task\n",
        "  ## async_mode: override the global async_mode settings for this task\n",
        "  prompt: \"prompts/summarize_descriptions.txt\"\n",
        "  max_length: 500\n",
        "\n",
        "claim_extraction:\n",
        "  ## llm: override the global llm settings for this task\n",
        "  ## parallelization: override the global parallelization settings for this task\n",
        "  ## async_mode: override the global async_mode settings for this task\n",
        "  # enabled: true\n",
        "  prompt: \"prompts/claim_extraction.txt\"\n",
        "  description: \"Any claims or facts that could be relevant to information discovery.\"\n",
        "  max_gleanings: 1\n",
        "\n",
        "community_reports:\n",
        "  ## llm: override the global llm settings for this task\n",
        "  ## parallelization: override the global parallelization settings for this task\n",
        "  ## async_mode: override the global async_mode settings for this task\n",
        "  prompt: \"prompts/community_report.txt\"\n",
        "  max_length: 2000\n",
        "  max_input_length: 8000\n",
        "\n",
        "cluster_graph:\n",
        "  max_cluster_size: 10\n",
        "\n",
        "embed_graph:\n",
        "  enabled: false # if true, will generate node2vec embeddings for nodes\n",
        "  # num_walks: 10\n",
        "  # walk_length: 40\n",
        "  # window_size: 2\n",
        "  # iterations: 3\n",
        "  # random_seed: 597832\n",
        "\n",
        "umap:\n",
        "  enabled: false # if true, will generate UMAP embeddings for nodes\n",
        "\n",
        "snapshots:\n",
        "  graphml: true\n",
        "  raw_entities: false\n",
        "  top_level_nodes: false\n",
        "\n",
        "local_search:\n",
        "  # text_unit_prop: 0.5\n",
        "  # community_prop: 0.1\n",
        "  # conversation_history_max_turns: 5\n",
        "  # top_k_mapped_entities: 10\n",
        "  # top_k_relationships: 10\n",
        "  # llm_temperature: 0 # temperature for sampling\n",
        "  # llm_top_p: 1 # top-p sampling\n",
        "  # llm_n: 1 # Number of completions to generate\n",
        "  # max_tokens: 12000\n",
        "\n",
        "global_search:\n",
        "  # llm_temperature: 0 # temperature for sampling\n",
        "  # llm_top_p: 1 # top-p sampling\n",
        "  # llm_n: 1 # Number of completions to generate\n",
        "  # max_tokens: 12000\n",
        "  # data_max_tokens: 12000\n",
        "  # map_max_tokens: 1000\n",
        "  # reduce_max_tokens: 2000\n",
        "  # concurrency: 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN8qYHZ9k9HL",
        "outputId": "70de9a45-46f9-4f09-ea46-1f67e3855c32"
      },
      "outputs": [],
      "source": [
        "!python3 -m graphrag.index --root ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question 0 written to questions/question_local_0.txt\n",
            "Question 1 written to questions/question_local_1.txt\n",
            "Question 2 written to questions/question_local_2.txt\n",
            "Question 3 written to questions/question_local_3.txt\n",
            "Question 4 written to questions/question_local_4.txt\n",
            "Question 5 written to questions/question_local_5.txt\n",
            "Question 6 written to questions/question_local_6.txt\n",
            "Question 7 written to questions/question_local_7.txt\n",
            "Question 8 written to questions/question_local_8.txt\n",
            "Question 9 written to questions/question_local_9.txt\n",
            "Question 10 written to questions/question_local_10.txt\n",
            "Question 11 written to questions/question_local_11.txt\n",
            "Question 12 written to questions/question_local_12.txt\n",
            "Question 13 written to questions/question_local_13.txt\n",
            "Question 14 written to questions/question_local_14.txt\n",
            "Question 15 written to questions/question_local_15.txt\n",
            "Question 16 written to questions/question_local_16.txt\n",
            "Question 17 written to questions/question_local_17.txt\n",
            "Question 18 written to questions/question_local_18.txt\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "\n",
        "# Define the base CLI command\n",
        "base_command = \"python3\"  # This is a simple example command that echoes the item\n",
        "method = \"local\"\n",
        "\n",
        "# Loop through the array and run the CLI command for each item\n",
        "for i, q in enumerate(questions):\n",
        "    args = f'python3 -m graphrag.query --root . --method {method} \"{q}\"'\n",
        "    filename = f\"questions/question_{method}_{i}.txt\"\n",
        "\n",
        "    # Open the file in write mode\n",
        "    with open(filename, \"w\") as file:\n",
        "        # Write the command and its output to the file\n",
        "        file.write(f\"{args} > {filename}\")\n",
        "\n",
        "    print(f\"Question {i} written to {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What are the most common migration paths for computer science professors between institutions?\",\n",
        "    \"Which regions in Brazil have experienced the highest influx of computer science professors?\",\n",
        "    \"Provide a summary of Author 3's academic migration path.\",\n",
        "    \"What are the common factors influencing the decision of professors to migrate to different institutions?\",\n",
        "    \"Is there a hierarchy in the professor's flow of academic migration between higher and lower prestige institutions?\",\n",
        "    \"Is there a hierarchy in Brazilian regions of flows of academic migration from higher to lower prestige institutions?\",\n",
        "    \"What is the percentage of professors that migrate from higher to lower prestigious institutions?\"\n",
        "    \"What is the percentage of professors that migrate from lower to higher prestigious institutions?\",\n",
        "    \"Does more prestigious institutions train more professors than less prestigious ones?\",\n",
        "    \"Does more prestigious Brazilian institutions train more professors in Brazil than less prestigious ones?\",\n",
        "    \"Does more prestigious institutions train more professors in their respective regions than less prestigious ones?\",\n",
        "    \"Is there a correlation between Prestige and International hiring of professors in Brazilian institutions?\",\n",
        "    \"Is there a correlation between Prestige and Self hiring, where a professor is hired as faculty in the institution where he was trained, in Brazilian institutions?\",\n",
        "    \"What are the most common institutions that professors in Brazil migrate from and to?\",\n",
        "    \"Are there any notable trends in the movement of professors between specific regions or cities in Brazil?\",\n",
        "    \"Which institutions in Brazil have the highest number of incoming or outgoing professors?\",\n",
        "    \"How do the CAPES scores of the programs of institutions correlate with the migration patterns of professors?\",\n",
        "    \"Are there any commonalities in the academic backgrounds (e.g., doctorate institutions) of professors who migrate to high-prestige institutions?\",\n",
        "    \"How do the academic backgrounds of professors at Pontifícia Universidade Católica do Rio Grande do Sul compare to those at other institutions in terms of their previous affiliations and current CAPES scores of their programs?\",\n",
        "    \"Which institutions act as hubs or central nodes in the academic migration network?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h32ebycCld4y",
        "outputId": "113c4b30-83f5-4ef3-e16c-14989159f46b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "INFO: Reading settings from settings.yaml\n",
            "creating llm client with {'api_key': 'REDACTED,len=56', 'type': \"openai_chat\", 'model': 'gpt-4o-mini', 'max_tokens': 4000, 'temperature': 0.0, 'top_p': 1.0, 'n': 1, 'request_timeout': 180.0, 'api_base': None, 'api_version': None, 'organization': None, 'proxy': None, 'cognitive_services_endpoint': None, 'deployment_name': None, 'model_supports_json': True, 'tokens_per_minute': 0, 'requests_per_minute': 0, 'max_retries': 10, 'max_retry_wait': 10.0, 'sleep_on_rate_limit_recommendation': True, 'concurrent_requests': 25}\n",
            "\n",
            "SUCCESS: Global Search Response: ## Types of Relationships Between Institutions\n",
            "\n",
            "The relationships between academic institutions can be categorized into several types, each contributing to the enhancement of academic collaboration, research output, and overall educational quality. Below are the key types of relationships identified in the dataset:\n",
            "\n",
            "### 1. **Collaborative Academic Relationships**\n",
            "Institutions often establish strong inter-institutional relationships that include collaborations on research projects, faculty exchanges, and joint academic programs. These collaborations enhance the academic environment and foster innovation through shared resources and expertise [Data: Reports (40, 62, 64, 71, 84)].\n",
            "\n",
            "### 2. **Professional Connections**\n",
            "Professional connections are formed when faculty members transition between institutions or when alumni take positions at different universities. This movement facilitates knowledge exchange and strengthens academic networks, as seen in the relationships where professors who obtained their doctorates from one institution are now teaching at another [Data: Reports (40, 62, 64, 71, 84)].\n",
            "\n",
            "### 3. **Geographical Relationships**\n",
            "Geographical proximity plays a significant role in fostering collaboration among institutions. Institutions located in the same region may collaborate more closely due to their accessibility, leading to partnerships that benefit local communities and enhance educational offerings [Data: Reports (40, 62, 64, 71, 84)].\n",
            "\n",
            "### 4. **International Academic Connections**\n",
            "Many institutions engage in international collaborations, allowing for the exchange of knowledge and research opportunities across borders. These global networks enhance the reputation and reach of the involved institutions, as seen in partnerships between Brazilian universities and institutions in the United States or Europe [Data: Reports (134, 76, 78)].\n",
            "\n",
            "### 5. **Shared Alumni Networks**\n",
            "Alumni networks serve as a vital link between institutions, where graduates from one institution take faculty positions at another. This interconnectedness enhances academic collaboration and influence across institutions, contributing to a robust academic community [Data: Reports (135, 74, 51, 19, 123, +more)].\n",
            "\n",
            "### 6. **Joint Research Initiatives**\n",
            "Collaborative research opportunities arise when institutions partner on specific projects, leading to joint publications and shared funding. This collaboration can significantly impact the research output and reputation of the involved institutions [Data: Reports (40, 62, 64, 71, 84)].\n",
            "\n",
            "### Conclusion\n",
            "The relationships between institutions are multifaceted and play a crucial role in enhancing the academic landscape. Through collaborative efforts, professional connections, geographical advantages, international partnerships, shared alumni networks, and joint research initiatives, institutions can foster a vibrant academic community that benefits both educators and students alike. These relationships not only enhance the quality of education but also contribute to the advancement of knowledge across various fields.\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python3 -m graphrag.query --root . --method global \"What are the types of relationships between institutions?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhdhn08vKDRm",
        "outputId": "4d8b520b-1134-4f52-b4bc-6bf927659ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "INFO: Reading settings from settings.yaml\n",
            "\n",
            "Loading Input (text)..\n",
            "INFO: Detecting language...\n",
            "\n",
            "INFO: Detected language: Portuguese\n",
            "\n",
            "INFO: Generating persona...\n",
            "\n",
            "INFO: Generated persona: You are an expert in academic migration analysis. You are skilled at mapping and interpreting the complex relationships and structures within academic communities, particularly in the context of migration trends. You are adept at helping people understand the dynamics of academic networks, facilitating collaboration, and identifying key stakeholders in the migration domain.\n",
            "\n",
            "INFO: Generating community report ranking description...\n",
            "\n",
            "INFO: Generated community report ranking description: A float score between 0-10 that represents the relevance of the text to academic migration analysis, including the mapping of academic networks, understanding migration trends among scholars, and the significance of institutional affiliations and performance metrics, with 1 being trivial or irrelevant and 10 being highly significant, impactful, and essential for understanding the dynamics of academic migration and collaboration.\n",
            "\n",
            "INFO: Generating entity types\n",
            "\n",
            "INFO: Generated entity types: ['author', 'institution', 'graduate program', 'CAPES score', 'doctorate']\n",
            "\n",
            "INFO: Generating entity relationship examples...\n",
            "\n",
            "INFO: Done generating entity relationship examples\n",
            "\n",
            "INFO: Generating entity extraction prompt...\n",
            "Failed to get encoding for cl100k_base when getting num_tokens_from_string. Fall back to default encoding cl100k_base\n",
            "Failed to get encoding for cl100k_base when getting num_tokens_from_string. Fall back to default encoding cl100k_base\n",
            "Failed to get encoding for cl100k_base when getting num_tokens_from_string. Fall back to default encoding cl100k_base\n",
            "Failed to get encoding for cl100k_base when getting num_tokens_from_string. Fall back to default encoding cl100k_base\n",
            "Failed to get encoding for cl100k_base when getting num_tokens_from_string. Fall back to default encoding cl100k_base\n",
            "\n",
            "INFO: Generated entity extraction prompt, stored in folder prompts\n",
            "\n",
            "INFO: Generating entity summarization prompt...\n",
            "\n",
            "INFO: Generated entity summarization prompt, stored in folder prompts\n",
            "\n",
            "INFO: Generating community reporter role...\n",
            "\n",
            "INFO: Generated community reporter role: A community analyst focused on academic migration trends, tasked with examining the relationships and affiliations of authors and institutions within the academic landscape of Brazil and beyond. This role will involve mapping the academic trajectories of authors, analyzing their doctoral origins, current positions, and the CAPES scores of their respective graduate programs. The findings will be compiled into a comprehensive report aimed at informing policymakers and educational stakeholders about the dynamics of academic migration, the distribution of academic talent, and the potential implications for higher education and research collaboration.\n",
            "\n",
            "INFO: Generating community summarization prompt...\n",
            "\n",
            "INFO: Generated community summarization prompt, stored in folder prompts\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python3 -m graphrag.prompt_tune --root . --domain \"academic migration\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
